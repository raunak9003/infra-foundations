## ğŸ§  Imagine this

You have an app running inside Kubernetes (EKS) â€” say, a small web app with a Pod.

Now, you want people to open it in a browser, like:
ğŸ‘‰ https://my-app.skyflow.com

But hereâ€™s the problem:

Kubernetes Pods donâ€™t have public IPs.
You canâ€™t directly reach them from the internet.

## ğŸ’¡ So What Do We Need?

We need something that:

Gets traffic from the outside world ğŸŒ

Sends it safely to the right Pod inside EKS ğŸ¯

That â€œsomethingâ€ is called a Load Balancer.

Itâ€™s like a receptionist ğŸ§â€â™€ï¸ sitting at the front door:

Knows where to send each visitor (request)

Balances load between all servers (Pods)

Can handle secure (HTTPS) connections

## âš™ï¸ But Who Creates This Load Balancer?

You could go to the AWS console and manually create an ALB/NLB,
then connect it to your Podsâ€¦
but thatâ€™s painful ğŸ˜© â€” lots of manual steps, and it breaks when Pods change.

So AWS made a helper:

AWS Load Balancer Controller ğŸ¯

Itâ€™s a Kubernetes controller that does this automatically.


## ğŸª„ What It Does

1ï¸âƒ£ You create a normal Kubernetes Service or Ingress
(just YAML â€” like every other resource).

2ï¸âƒ£ The AWS Load Balancer Controller sees that,
and says:
â€œOh, you want to expose this app? Let me handle it.â€

3ï¸âƒ£ It goes to AWS and creates a real Load Balancer (ALB or NLB).

Adds target groups for your Pods

Adds listeners for HTTP/HTTPS

Sets up health checks

Updates everything when Pods change

4ï¸âƒ£ You get a working AWS Load Balancer â€” automatically! ğŸª„



ğŸ” Example

You create:
```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-app
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: nlb
spec:
  type: LoadBalancer
  selector:
    app: my-app
  ports:
    - port: 80
```

Then the controller:

Creates an NLB in AWS

Connects it to your appâ€™s Pods

Gives you a DNS name (like a1b2c3d4.us-west-2.elb.amazonaws.com)

Now you can open that URL â€” it reaches your app ğŸ‰

## ğŸ§± Step 1: What NodePort (you said â€œnode IPâ€) actually does

When you set a Kubernetes Service like this:
```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-app
spec:
  type: NodePort
  selector:
    app: my-app
  ports:
  - port: 80
    nodePort: 30080
```

It means:

Kubernetes opens port 30080 on every worker node in your cluster.

Any request coming to a nodeâ€™s IP on port 30080 is forwarded to your appâ€™s Pod (through kube-proxy / iptables).

âœ… So yes â€” you can access your app using:

http://<node-public-ip>:30080


âš ï¸ Step 2: The problem with NodePort

That works technically, but itâ€™s not practical or safe for production.

Letâ€™s see why ğŸ‘‡

âŒ Problem 1 â€” No single entry point

You might have 3 nodes today, 6 tomorrow.
Which nodeâ€™s IP do you give your users? ğŸ¤”
If one node goes down â†’ users hitting that IP get errors.

Thereâ€™s no automatic balancing â€” youâ€™re managing node IPs manually.

âŒ Problem 2 â€” Node IPs arenâ€™t stable

AWS EC2 nodes (in EKS) are ephemeral:

When nodes are scaled up/down, their IPs change.

New nodes join with new IPs.

So your app endpoint keeps changing â†’ users canâ€™t rely on a fixed address.

âŒ Problem 3 â€” No HTTPS, WAF, or TLS

NodePort is just raw TCP.
It doesnâ€™t:

Terminate HTTPS

Attach AWS ACM or PCA certificates

Support WAF, Shield, or health checks

Distribute traffic evenly

Basically, itâ€™s a dumb forwarder â€” no smart routing or security.

ğŸ§  Step 3: What Load Balancer fixes

Now enter: AWS Load Balancer Controller + LoadBalancer Service

When you set:

type: LoadBalancer


Kubernetes asks AWS to create:

An NLB (for TCP) or

An ALB (for HTTP/HTTPS)

Then AWS automatically:

Gives you a single DNS name (like abc123.elb.amazonaws.com)

Distributes traffic across all nodes/pods

Auto-updates when nodes scale up/down

Handles TLS certificates and security

Keeps the endpoint stable even when cluster topology changes
